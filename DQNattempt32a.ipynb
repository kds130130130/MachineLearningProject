{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010046518914696807\n"
     ]
    }
   ],
   "source": [
    "epsilonTest = 1\n",
    "decayTest = 0.99977\n",
    "episodesTest = 20000\n",
    "for i in range(episodesTest):\n",
    "    epsilonTest*=decayTest\n",
    "print(epsilonTest)\n",
    "#For run continuations, define:\n",
    "#LOAD_MODEL, epsilon, decay, episodeStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os #Setup which GPU to use before everything else to avoid conflicts\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import datetime\n",
    "import csv\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from PIL import Image, ImageDraw\n",
    "import keras.backend.tensorflow_backend as backend\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten, Input\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "#Misc Parameters\n",
    "USE_SCOTT_MODEL = False\n",
    "CHECK_DQN_INPUT = False\n",
    "ID = int(time.time())\n",
    "LOAD_MODEL = None #Copy paste full model name and enclose it in quotes to load models of all agents. Otherwise, set to None\n",
    "SAVE_MODEL_EVERY = 1000 #Episodes. Set to -1 for no saves\n",
    "MODEL_NAME = 'Merge_Dynamic' # Name of all the saved files that are related to this run\n",
    "RANDOM_SEED = -1 # Sets seed for all random number generators. -1 to turn off\n",
    "ACTION_SPACE_SIZE = 4\n",
    "\n",
    "#Model Settings\n",
    "DISCOUNT = 0.90\n",
    "REPLAY_MEMORY_SIZE = 20_000  # How many last steps to keep for model training \n",
    "MIN_REPLAY_MEMORY_SIZE = 1200 # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 32  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 10  # Terminal states (end of episodes)\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Agent View Settings\n",
    "REMOVE_PLAYER_FROM_LOCAL_VIEW = True\n",
    "WALLS_BLOCK_VIEW = True\n",
    "VIEW_TYPE = \"Merge\" # Relative or Local or Merge\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # starting epsilon value. Not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.9998\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "# Record Parameters\n",
    "# Saved to text file titled the same as the logs\n",
    "RECORD_PARAMETERS = True\n",
    "str_model_summary = \"No Summary\"\n",
    "\n",
    "PRINT_CSV = True\n",
    "\n",
    "# Environment settings\n",
    "START_EPISODE = 1\n",
    "END_EPISODE = 25000\n",
    "OBS_RANGE = 3\n",
    "SIZE_X = 35#Environment width\n",
    "SIZE_Y = 20#Environment height\n",
    "EPISODE_STEP_LIMIT = 300\n",
    "PLAYER_COUNT = 8\n",
    "TRASH_COUNT = 30\n",
    "AGENT_SPAWN = \"4Room Center\" # \"Random\", \"Center\", \"Corridor\", \"Each Room\", \"Custom\", \"4Room Center\",\"4Room Center 8Agent Static\"\n",
    "TASK_SPAWN = \"4Room Each Room\" # \"Random\", \"Not Center\", \"Along Walls\", \"Not Corridor\", \"Not Each Room\", \"Custom\", \"4Room Each Room\", \"4Room 1Room Doubled\"\n",
    "WALL_ARRANGEMENT = \"Four Room Block Wall\" # \"Six Rooms\", \"Custom\", \"Four Room Block Wall\"\n",
    "\n",
    "#Key settings (the number to represent each thing in the DQN matrix)\n",
    "PLAYER_N = 1  # player key \n",
    "TRASH_N = 1  # trash key\n",
    "OTHERS_N = 1 # Other players key\n",
    "BLIND_N = -1 # key for areas that can't be seen due to a wall\n",
    "\n",
    "#Reward Scheme\n",
    "MOVE_PENALTY = 0 #make negative\n",
    "WALL_COLLISION_PENALTY = 0 #make negative\n",
    "AGENT_COLLISION_PENALTY = 0 #make negative\n",
    "TRASH_REWARD = 1\n",
    "\n",
    "#DQN input shape settings\n",
    "CHANNEL_COUNT = 4 #Depending on view type, certain channels are removed upon DQN input\n",
    "PLAYER_CHANNEL = 0\n",
    "OTHERS_CHANNEL = 1\n",
    "TRASH_CHANNEL = 2\n",
    "BLIND_CHANNEL = 3\n",
    "OBSERVATION_SPACE_VALUES = (1, 1, 1)# Actual values assigned on init of environment\n",
    "OBSERVATION_SPACE_VALUES_TWO = (1, 1, 1)# For merged view\n",
    "\n",
    "# Determines the DQN input shape\n",
    "localViewSize = OBS_RANGE*2+1\n",
    "relativeViewSizeX = SIZE_X\n",
    "relativeViewSizeY = SIZE_Y\n",
    "if VIEW_TYPE == \"Local\":\n",
    "    if REMOVE_PLAYER_FROM_LOCAL_VIEW:\n",
    "        OBSERVATION_SPACE_VALUES = (localViewSize, localViewSize, CHANNEL_COUNT-1)\n",
    "    else:\n",
    "        OBSERVATION_SPACE_VALUES = (localViewSize, localViewSize, CHANNEL_COUNT)\n",
    "elif VIEW_TYPE == \"Relative\":\n",
    "    OBSERVATION_SPACE_VALUES = (relativeViewSizeX, relativeViewSizeY, CHANNEL_COUNT)\n",
    "elif VIEW_TYPE == \"Merge\":\n",
    "    if REMOVE_PLAYER_FROM_LOCAL_VIEW:\n",
    "        OBSERVATION_SPACE_VALUES = (localViewSize, localViewSize, CHANNEL_COUNT-2)\n",
    "    else:\n",
    "        OBSERVATION_SPACE_VALUES = (localViewSize, localViewSize, CHANNEL_COUNT-1)\n",
    "    OBSERVATION_SPACE_VALUES_TWO = (relativeViewSizeX, relativeViewSizeY, 2)\n",
    "    \n",
    "#Scott's Shared Memory Model Variables\n",
    "if USE_SCOTT_MODEL:\n",
    "    GLOBAL_REPLAY_MEMORY = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "    ACTION_SPACE_SIZE += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roomba:\n",
    "    # Size = size of the environment; e.g., if NxN grid, size=N\n",
    "    def __init__(self, obsRange=3, viewType = \"none\", ID = -1):       \n",
    "        self.ID = ID\n",
    "        self.x = np.random.randint(0, SIZE_X)# Xpos of the agent\n",
    "        self.y = np.random.randint(0, SIZE_Y)# Ypos of the agent\n",
    "        self.xBefore = -1# Last Xpos of the agent\n",
    "        self.yBefore = -1# Last Ypos of the agent\n",
    "        self.obsRange = obsRange\n",
    "        self.stepsTaken = 0\n",
    "        self.agentCollisionCount = 0\n",
    "        self.wallCollisionCount = 0\n",
    "        self.tasksCompleted = 0\n",
    "        self.reward = 0\n",
    "        self.viewType = viewType\n",
    "        self.event = \"nothing\"\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"Roomba ({self.x}, {self.y})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choice):\n",
    "        '''\n",
    "        Gives us 4 total movement options. (0,1,2,3)\n",
    "        '''\n",
    "        if choice == 0:\n",
    "            self.move(x=0, y=1)#up\n",
    "        elif choice == 1:\n",
    "            self.move(x=1, y=0)#right\n",
    "        elif choice == 2:\n",
    "            self.move(x=0, y=-1)#down\n",
    "        elif choice == 3:\n",
    "            self.move(x=-1, y=0)#left\n",
    "        self.stepsTaken += 1\n",
    "        \n",
    "    def move(self, x=False, y=False):\n",
    "        #Save position from previous move\n",
    "        self.yBefore = self.y\n",
    "        self.xBefore = self.x\n",
    "        \n",
    "        # If no value for x, no change for x\n",
    "        if not x:\n",
    "            self.x += 0\n",
    "        else:\n",
    "            self.x += x\n",
    "\n",
    "        # If no value for y, no change for y\n",
    "        if not y:\n",
    "            self.y += 0\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "    # Reverts agent to previous position in the case of colision with other agents\n",
    "    def undo(self):\n",
    "        self.y = self.yBefore\n",
    "        self.x = self.xBefore\n",
    "    \n",
    "    # Returns True if agent has not changed position from last step\n",
    "    def hasNotMoved(self):\n",
    "        return self.xBefore == self.x and self.yBefore == self.y\n",
    "    \n",
    "class Trash:\n",
    "    # Size = size of the environment; e.g., if NxN grid, size=N\n",
    "    def __init__(self):\n",
    "        self.x = np.random.randint(0, SIZE_X)\n",
    "        self.y = np.random.randint(0, SIZE_Y)\n",
    "        self.count = 0\n",
    "            \n",
    "    def __str__(self):\n",
    "        return f\"Trash ({self.x}, {self.y})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to manage matrix for DQN input and rendering \n",
    "class MapManager:\n",
    "    def __init__(self):\n",
    "        self.sizeX = SIZE_X\n",
    "        self.sizeY = SIZE_Y\n",
    "        self.matrix = np.zeros((SIZE_X, SIZE_Y, CHANNEL_COUNT), dtype=np.int8)\n",
    "        self.playerList = []\n",
    "        self.trashList = []\n",
    "        self.blockWallList = []\n",
    "        \n",
    "    #Updates the map matrix with given player coordinates and trash coordinates\n",
    "    def update(self, playerList, trashList, blockWallList):\n",
    "        self.matrix = np.zeros((self.sizeX, self.sizeY, CHANNEL_COUNT), dtype=np.int8)\n",
    "        for eachPlayer in playerList:\n",
    "            self.addOtherPlayer(eachPlayer.x, eachPlayer.y)\n",
    "        for eachTrash in trashList:\n",
    "            self.addTask(eachTrash.x, eachTrash.y)\n",
    "        for eachCoord in blockWallList:\n",
    "            x = eachCoord[0]\n",
    "            y = eachCoord[1]\n",
    "            #self.addBlockWall(x,y)\n",
    "        self.playerList = playerList\n",
    "        self.trashList = trashList\n",
    "        self.blockWallList = blockWallList    \n",
    "    \n",
    "    # Adds the player that's supposed to have this obvservation\n",
    "    # Needs specific location\n",
    "    def addPlayer(self, playerX, playerY):\n",
    "        self.matrix[playerX][playerY][PLAYER_CHANNEL] = PLAYER_N\n",
    "    \n",
    "    #Adds a player that isn't this player\n",
    "    # Needs specific location\n",
    "    def addOtherPlayer(self, otherX, otherY):\n",
    "        self.matrix[otherX][otherY][OTHERS_CHANNEL] = OTHERS_N\n",
    "    \n",
    "    #Adds a task item to the specified location\n",
    "    def addTask(self, taskX, taskY):\n",
    "        self.matrix[taskX][taskY][TRASH_CHANNEL] = TRASH_N\n",
    "    \n",
    "    def addBlockWall(self, blockWallX, blockWallY):\n",
    "        self.matrix[blockWallX][blockWallY][BLIND_CHANNEL] = BLIND_N\n",
    "    \n",
    "    #Adds an area the agent cannot see\n",
    "    def addBlindSpot(self, blindX, blindY):\n",
    "        if blindX >= 0 and blindX < SIZE_X and blindY >=0 and blindY < SIZE_Y:\n",
    "            self.matrix[blindX][blindY][PLAYER_CHANNEL] = 0\n",
    "            self.matrix[blindX][blindY][OTHERS_CHANNEL] = 0\n",
    "            self.matrix[blindX][blindY][TRASH_CHANNEL] = 0\n",
    "            self.matrix[blindX][blindY][BLIND_CHANNEL] = BLIND_N\n",
    "    \n",
    "    #Removes everything from the observation\n",
    "    def clear(self):\n",
    "        self.matrix = np.zeros((self.sizeX, self.sizeY, CHANNEL_COUNT), dtype=np.int8)\n",
    "    \n",
    "    #Returns copy of map\n",
    "    #Useful because this passes by value...\n",
    "    #not by reference which python might do without this method\n",
    "    def getEnvironment(self):\n",
    "        return copy.deepcopy(self.matrix)\n",
    "    \n",
    "    #Returns what the agent sees given its viewType as a numpy array\n",
    "    def getView(self, obsRange, viewType, agentID = -1, removePlayerPos = False, blindSpotList = None):\n",
    "        for eachPlayer in self.playerList:\n",
    "            if agentID == eachPlayer.ID:\n",
    "                x = eachPlayer.x\n",
    "                y = eachPlayer.y\n",
    "        \n",
    "        # To avoid typing \"self.\" many times\n",
    "        sizeX = self.sizeX\n",
    "        sizeY = self.sizeY\n",
    "        fullEnv = self.getEnvironment()\n",
    "        \n",
    "        #Changes main player on map to be the target agent\n",
    "        fullEnv[x][y][PLAYER_CHANNEL] = PLAYER_N\n",
    "        fullEnv[x][y][OTHERS_CHANNEL] = 0\n",
    "        \n",
    "        if blindSpotList != None:\n",
    "            for eachSpot in blindSpotList:\n",
    "                blindX = eachSpot[0]\n",
    "                blindY = eachSpot[1]\n",
    "                if blindX >= 0 and blindX < SIZE_X and blindY >=0 and blindY < SIZE_Y:\n",
    "                    fullEnv[blindX][blindY][PLAYER_CHANNEL] = 0\n",
    "                    fullEnv[blindX][blindY][OTHERS_CHANNEL] = 0\n",
    "                    fullEnv[blindX][blindY][TRASH_CHANNEL] = 0\n",
    "                    fullEnv[blindX][blindY][BLIND_CHANNEL] = BLIND_N\n",
    "                \n",
    "        #np.set_printoptions(threshold=sys.maxsize)\n",
    "        #np.set_printoptions(threshold=1000)\n",
    "        if viewType == \"Local\":\n",
    "            # Creates a large matrix called \"canvas\" with -1 for all its values\n",
    "            canvas = np.zeros((2*obsRange+sizeX+1, 2*obsRange+sizeY+1, CHANNEL_COUNT), dtype=np.int8)\n",
    "            canvas = canvas-1\n",
    "\n",
    "            # Pastes the full environment onto the center of the canvas\n",
    "            canvas[obsRange:obsRange+sizeX,obsRange:obsRange+sizeY,:]=fullEnv\n",
    "            \n",
    "            # Returns a matrix containing the area around the agent\n",
    "            viewMatrix=canvas[x:x+2*obsRange+1,y:y+2*obsRange+1,:]\n",
    "            \n",
    "            #viewMatrix = np.delete(viewMatrix, BLIND_CHANNEL, axis=2)#bookmark maybe need again\n",
    "            if removePlayerPos:\n",
    "                viewMatrix = np.delete(viewMatrix, PLAYER_CHANNEL, axis=2)\n",
    "                \n",
    "        elif viewType == \"Relative\":\n",
    "            #creates 3D matrix the same size as env full of zeroes\n",
    "            viewMatrix = np.zeros((sizeX,sizeY,CHANNEL_COUNT), dtype=np.int8)\n",
    "            \n",
    "            #Finds the borders of the agent's view\n",
    "            left = x-obsRange\n",
    "            right = x+obsRange+1\n",
    "            up = y-obsRange\n",
    "            down = y+obsRange+1\n",
    "            #Keeps agent view from going outside environment\n",
    "            if left < 0:\n",
    "                left=0\n",
    "            if right > sizeX:\n",
    "                right = sizeX\n",
    "            if up < 0:\n",
    "                up = 0\n",
    "            if down > sizeY:\n",
    "                down = sizeY\n",
    "                \n",
    "            #Gets the agent's view as a matrix\n",
    "            agentView=fullEnv[left:right,up:down,:]\n",
    "            \n",
    "            #Pastes the agents view onto the zero matrix\n",
    "            viewMatrix[left:right,up:down,:] = agentView\n",
    "            \n",
    "            #viewMatrix = np.delete(viewMatrix, BLIND_CHANNEL, axis=2) #bookmark maybe need again later\n",
    "            \n",
    "        elif viewType == \"Merge\":\n",
    "            ######### Local View\n",
    "            # Creates a large matrix called \"canvas\" with -1 for all its values\n",
    "            canvas = np.zeros((2*obsRange+sizeX+1, 2*obsRange+sizeY+1, CHANNEL_COUNT), dtype=np.int8)\n",
    "            canvas = canvas-1\n",
    "\n",
    "            # Pastes the full environment onto the center of the canvas\n",
    "            canvas[obsRange:obsRange+sizeX,obsRange:obsRange+sizeY,:]=fullEnv\n",
    "\n",
    "            # Returns a matrix containing the area around the agent\n",
    "            viewMatrix=canvas[x:x+2*obsRange+1,y:y+2*obsRange+1,:]\n",
    "            \n",
    "            viewMatrix = np.delete(viewMatrix, BLIND_CHANNEL, axis=2)#blind channel added to relative view portion\n",
    "            if removePlayerPos:\n",
    "                viewMatrix = np.delete(viewMatrix, PLAYER_CHANNEL, axis=2)\n",
    "                \n",
    "            #####################\n",
    "            \n",
    "            ######### Relative View\n",
    "            #creates 3D matrix the same size as env full of zeroes\n",
    "            viewMatrix2 = np.zeros((sizeX,sizeY,CHANNEL_COUNT), dtype=np.int8)\n",
    "            \n",
    "            #Finds the borders of the agent's view\n",
    "            left = x-obsRange\n",
    "            right = x+obsRange+1\n",
    "            up = y-obsRange\n",
    "            down = y+obsRange+1\n",
    "            #Keeps agent view from going outside environment\n",
    "            if left < 0:\n",
    "                left=0\n",
    "            if right > sizeX:\n",
    "                right = sizeX\n",
    "            if up < 0:\n",
    "                up = 0\n",
    "            if down > sizeY:\n",
    "                down = sizeY\n",
    "                \n",
    "            #Gets the agent's view as a matrix\n",
    "            agentView=fullEnv[left:right,up:down,:]\n",
    "            \n",
    "            #Pastes the agents view onto the zero matrix\n",
    "            viewMatrix2[left:right,up:down,:] = agentView\n",
    "            viewMatrix2 = np.delete(viewMatrix2, TRASH_CHANNEL, axis=2)\n",
    "            viewMatrix2 = np.delete(viewMatrix2, OTHERS_CHANNEL, axis=2)#Order of deletion matters\n",
    "            #####################\n",
    "            return [viewMatrix, viewMatrix2]\n",
    "        return viewMatrix\n",
    "    \n",
    "    def printView(self, viewMatrix):#bookmark untested\n",
    "        viewCopy = copy.deepcopy(viewMatrix)\n",
    "        if viewCopy.ndim >= 3:\n",
    "            viewCopy = np.swapaxes(viewCopy,1,2)\n",
    "        if viewCopy.ndim >= 2:\n",
    "            viewCopy = np.swapaxes(viewCopy,0,1)\n",
    "        print(viewCopy)\n",
    "    \n",
    "    # Print observation array #obsolete due to added channels\n",
    "    def printEnv(self):\n",
    "        viewCopy = self.getEnvironment()\n",
    "        if viewCopy.ndim >= 3:\n",
    "            viewCopy = np.swapaxes(viewCopy,1,2)\n",
    "        if viewCopy.ndim >= 2:\n",
    "            viewCopy = np.swapaxes(viewCopy,0,1)\n",
    "        print(viewCopy)\n",
    "        \n",
    "class WallManager:\n",
    "    def __init__(self):\n",
    "        self.wallList = []\n",
    "        self.blockWallList = []\n",
    "    \n",
    "    def getWallList(self):\n",
    "        return self.wallList\n",
    "    \n",
    "    def getWallsWithinView(self, agentX, agentY, obsRange):\n",
    "        viewEdgeRight = agentX + obsRange\n",
    "        viewEdgeLeft = agentX - obsRange\n",
    "        viewEdgeTop = agentY + obsRange\n",
    "        viewEdgeBot = agentY - obsRange\n",
    "        wallsWithinViewList = []\n",
    "        for eachPair in self.wallList:\n",
    "            x1 = eachPair[0][0]\n",
    "            y1 = eachPair[0][1]\n",
    "            x2 = eachPair[1][0]\n",
    "            y2 = eachPair[1][1]\n",
    "            if (max(x1,x2) <= viewEdgeRight and \n",
    "                min(x1,x2) >= viewEdgeLeft and \n",
    "                max(y1,y2) <= viewEdgeTop and\n",
    "                min(y1,y2) >= viewEdgeBot):\n",
    "                wallsWithinViewList.append(eachPair)\n",
    "        return wallsWithinViewList\n",
    "    \n",
    "    # Returns true if wall exists at the given coordinates\n",
    "    # Returns false otherwise\n",
    "    def wallExists(self, x1, y1, x2, y2):\n",
    "        if len(self.wallList) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            for eachPair in self.wallList:\n",
    "                pairX1 = eachPair[0][0]\n",
    "                pairY1 = eachPair[0][1]\n",
    "                pairX2 = eachPair[1][0]\n",
    "                pairY2 = eachPair[1][1]\n",
    "                if ((x1 == pairX1 and y1 == pairY1) and (x2 == pairX2 and y2 == pairY2)\n",
    "                   or (x1 == pairX2 and y1 == pairY2) and (x2 == pairX1 and y2 == pairY1)):\n",
    "                    return True\n",
    "            return False\n",
    "    \n",
    "    def addBlock(self, x = -1, y = -1):\n",
    "        self.addWall(x-1,y+1,\"Horizontal\",1)\n",
    "        self.addWall(x-1,y+1,\"Vertical\",1)\n",
    "        self.addWall(x-1,y,\"Horizontal\",1)\n",
    "        self.addWall(x,y+1,\"Vertical\",1)        \n",
    "        self.blockWallList.append([x,y])\n",
    "    \n",
    "    #Checks if a block exists in given x and y position\n",
    "    #if x is not defined, checks if wall is present in a horizontal line at given y\n",
    "    #if y is not defined, checks if wall is present in a vertical line at given x\n",
    "    #if neither are defined, checks whole environment for a wall\n",
    "    def blockExists(self, x=None, y=None):\n",
    "        for blockWall in self.blockWallList:\n",
    "            xWall = blockWall[0]\n",
    "            yWall = blockWall[1]\n",
    "            xMatch = False\n",
    "            yMatch = False\n",
    "            if (x is None) or (xWall == x):\n",
    "                xMatch = True\n",
    "            if (y is None) or (yWall == y):\n",
    "                yMatch = True\n",
    "            if xMatch and yMatch:\n",
    "                return True\n",
    "        return False    \n",
    "    \n",
    "    # Add wall or a line of walls in the designated coordinates\n",
    "    # xTL and yTL are the coordinates to the top left portion of the wall\n",
    "    # shape determines whether the wall will be drawn vertically or horizontally from point (xTL, yTL)\n",
    "    # length determines the length of the wall\n",
    "    def addWall(self, xTL=-1, yTL=-1, shape = \"-1\", length = -1):    \n",
    "        if shape == \"Vertical\":\n",
    "            for y in range(yTL-1,yTL-length-1,-1):\n",
    "                self.wallList.append(([xTL, y], [xTL+1, y]))\n",
    "        if shape == \"Horizontal\":\n",
    "            for x in range(xTL+1,xTL+length+1):\n",
    "                self.wallList.append(([x, yTL], [x, yTL-1]))\n",
    "        #print(self.wallList)\n",
    "    \n",
    "class SpawnManager:\n",
    "    def __init__(self):\n",
    "        self.trashSpawnMap = np.zeros((SIZE_X, SIZE_Y))\n",
    "        self.playerSpawnMap = np.zeros((SIZE_X, SIZE_Y))\n",
    "        self.playerIDspawnMap = np.zeros((SIZE_X, SIZE_Y))\n",
    "        self.playerIDspawnMap += -1\n",
    "    \n",
    "    # Makes 1 attempt at spawning a certain thing (\"Player\" or \"Trash\") at a certain location\n",
    "    # returns True if spawn was successful, False otherwise\n",
    "    def trySpawn(self, x=-1, y=-1, thing=\"-1\", playerID = -1):\n",
    "        randy = random.randint(1,101)\n",
    "        if playerID != self.playerIDspawnMap[x][y] and \"Static\" in AGENT_SPAWN:\n",
    "            return False\n",
    "        elif ((thing == \"Player\" and randy <= self.playerSpawnMap[x][y]) or \n",
    "        (thing == \"Trash\" and randy <= self.trashSpawnMap[x][y])):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def updateSpawnPlayerID(self, x, y, playerID):\n",
    "        self.playerIDspawnMap[x][y] = playerID\n",
    "    \n",
    "    # Changes the probability of a certain thing (\"Player\", \"Trash\") to spawn at a certain location\n",
    "    # chance can range from 0 to 100, describing the percent probability of something spawning there\n",
    "    def updateSpawn(self, x, y, thing, chance):\n",
    "        if thing == \"Player\":\n",
    "            self.playerSpawnMap[x][y] = chance\n",
    "        if thing == \"Trash\":\n",
    "            self.trashSpawnMap[x][y] = chance\n",
    "\n",
    "class BlindSpotManager:\n",
    "    # Returns list of coordinates that the agent cannot see due to walls blocking view\n",
    "    def getBlindSpots(self, agentX, agentY, obsRange, wallManager):\n",
    "        visibleWallList = wallManager.getWallsWithinView(agentX, agentY, obsRange)\n",
    "        totalBlindSpotList = []\n",
    "        for eachVisibleWall in visibleWallList:\n",
    "            # Get list of blind spots made by each wall\n",
    "            blindSpotList = self.getBlindSpotsForOneWall(agentX, agentY, obsRange, eachVisibleWall)\n",
    "            \n",
    "            # Append all blind spots to 1 list\n",
    "            totalBlindSpotList = totalBlindSpotList + blindSpotList\n",
    "            \n",
    "        #Remove duplicates\n",
    "        totalBlindSpotList = list(dict.fromkeys(totalBlindSpotList))\n",
    "\n",
    "        return totalBlindSpotList\n",
    "    \n",
    "    def getBlindSpotsForOneWall(self, agentX, agentY, agentObs, wallCoord):\n",
    "        scale = 10\n",
    "        \n",
    "        obsRange = agentObs\n",
    "        x1 = wallCoord[0][0]\n",
    "        y1 = wallCoord[0][1]\n",
    "        x2 = wallCoord[1][0]\n",
    "        y2 = wallCoord[1][1]\n",
    "        if x1 == x2:\n",
    "            orientation = \"Horizontal\"\n",
    "        elif y1==y2:\n",
    "            orientation = \"Vertical\"\n",
    "        \n",
    "        \n",
    "        verticalAlignment = self.wallLocationVertical(agentY, wallCoord)\n",
    "        horizontalAlignment = self.wallLocationHorizontal(agentX, wallCoord)\n",
    "\n",
    "        # Find the center of the grid space for the agent\n",
    "        agentCenterX = agentX*scale-(scale/2)\n",
    "        agentCenterY = agentY*scale-(scale/2)\n",
    "        \n",
    "        # Find the edges of the wall\n",
    "        if orientation == \"Horizontal\":\n",
    "            edgeX1 = x1*scale\n",
    "            edgeY1 = min(y1, y2)*scale\n",
    "            edgeX2 = x1*scale-scale\n",
    "            edgeY2 = min(y1, y2)*scale\n",
    "        if orientation == \"Vertical\":\n",
    "            edgeX1 = min(x1,x2)*scale\n",
    "            edgeY1 = y1*scale\n",
    "            edgeX2 = min(x1,x2)*scale\n",
    "            edgeY2 = y1*scale-scale\n",
    "        \n",
    "        slope1 = (edgeY1-agentCenterY)/(edgeX1-agentCenterX)\n",
    "        slope2 = (edgeY2-agentCenterY)/(edgeX2-agentCenterX)\n",
    "        \n",
    "        #The 'b' in y=mx+b\n",
    "        b1 = self.findYintercept(edgeX1, edgeY1, slope1)\n",
    "        b2 = self.findYintercept(edgeX2, edgeY2, slope2)\n",
    "        \n",
    "        #minSlope is always more clockwise than maxSlope\n",
    "        ccwSlope = max(slope1, slope2)\n",
    "        cwSlope = min(slope1, slope2)\n",
    "        \n",
    "        #Assign 'b' values in y=mx+b to match the ccw and cw slopes\n",
    "        if ccwSlope == slope1:\n",
    "            ccwB = b1\n",
    "            cwB = b2\n",
    "        else:\n",
    "            ccwB = b2\n",
    "            cwB = b1\n",
    "            \n",
    "        blindCoords = []\n",
    "        #Iterate over every xy coordinate with agent's view range\n",
    "        for y in range(agentY-obsRange, agentY+obsRange+1):\n",
    "            for x in range(agentX-obsRange, agentX+obsRange+1):\n",
    "                centerX = x*scale-scale/2\n",
    "                centerY = y*scale-scale/2\n",
    "                cwStatus = self.aboveOrBelowLine(centerX,centerY,cwSlope, cwB)\n",
    "                ccwStatus = self.aboveOrBelowLine(centerX,centerY,ccwSlope, ccwB)\n",
    "                \n",
    "                if horizontalAlignment == \"left\":\n",
    "                    if (cwStatus == \"below\" or cwStatus == \"neither\") and (ccwStatus == \"above\" or ccwStatus == \"neither\"):            \n",
    "                        if x <= min(x1, x2):\n",
    "                            if verticalAlignment == \"above\":\n",
    "                                if y >= max(y1,y2):\n",
    "                                    blindCoords.append((x,y))\n",
    "                            elif verticalAlignment == \"below\":\n",
    "                                if y <= min(y1,y2):\n",
    "                                    blindCoords.append((x,y))\n",
    "                            else:\n",
    "                                blindCoords.append((x,y))\n",
    "                    \n",
    "                if horizontalAlignment == \"right\":\n",
    "                    if (cwStatus == \"above\" or cwStatus == \"neither\") and (ccwStatus == \"below\" or ccwStatus == \"neither\"):\n",
    "                        if x >= max(x1, x2):\n",
    "                            if verticalAlignment == \"above\":\n",
    "                                if y >= max(y1,y2):\n",
    "                                    blindCoords.append((x,y))\n",
    "                            elif verticalAlignment == \"below\":\n",
    "                                if y <= min(y1,y2):\n",
    "                                    blindCoords.append((x,y))\n",
    "                            else:\n",
    "                                blindCoords.append((x,y))\n",
    "                \n",
    "                if horizontalAlignment == \"neutral\":\n",
    "                    if verticalAlignment == \"above\":\n",
    "                        if (cwStatus == \"above\" or cwStatus == \"neither\") and (ccwStatus == \"above\" or ccwStatus == \"neither\"):\n",
    "                            if y >= max(y1, y2):\n",
    "                                blindCoords.append((x,y))\n",
    "                    if verticalAlignment == \"below\":\n",
    "                        if (cwStatus == \"below\" or cwStatus == \"neither\") and (ccwStatus == \"below\" or ccwStatus == \"neither\"):\n",
    "                            if y <= min(y1, y2):\n",
    "                                blindCoords.append((x,y))\n",
    "        return blindCoords\n",
    "    \n",
    "    def findYintercept(self, x, y, slope):\n",
    "        return y - slope*x\n",
    "    \n",
    "    # Checks if the given x,y coordinates are above or below a line\n",
    "    # line is defined as y=mx+b where m=slope and b=b in the arguments\n",
    "    def aboveOrBelowLine(self, x, y, slope, b):\n",
    "        lineThickness = 0#higher the number, more likely \"neither\" will be returned when the given coordinates are closer to the line\n",
    "        \n",
    "        lineY = (slope*x)+b#y=mx+b\n",
    "        \n",
    "        if abs(y-lineY)<=lineThickness:\n",
    "            return \"neither\"\n",
    "        if y > lineY:\n",
    "            return \"above\"\n",
    "        if y < lineY:\n",
    "            return \"below\"\n",
    "        \n",
    "    def wallLocationHorizontal(self, agentX, wallCoord):\n",
    "        x1 = wallCoord[0][0]\n",
    "        x2 = wallCoord[1][0]\n",
    "        \n",
    "        if x1==x2:\n",
    "            if x1 < agentX:\n",
    "                return \"left\"\n",
    "            if x1 > agentX:\n",
    "                return \"right\"\n",
    "            if x1 == agentX:\n",
    "                return \"neutral\"\n",
    "        else:\n",
    "            if max(x1,x2) > agentX:\n",
    "                return \"right\"\n",
    "            elif min(x1,x2) < agentX:\n",
    "                return \"left\"\n",
    "    \n",
    "    def wallLocationVertical(self, agentY, wallCoord):\n",
    "        y1 = wallCoord[0][1]\n",
    "        y2 = wallCoord[1][1]\n",
    "        \n",
    "        if y1==y2:\n",
    "            if y1 < agentY:\n",
    "                return \"below\"\n",
    "            if y1 > agentY:\n",
    "                return \"above\"\n",
    "            if y1 == agentY:\n",
    "                return \"neutral\"\n",
    "        else:\n",
    "            if max(y1,y2) > agentY:\n",
    "                return \"above\"\n",
    "            elif min(y1,y2) < agentY:\n",
    "                return \"below\"\n",
    "            \n",
    "class CSVmanager:\n",
    "    def __init__(self, noBlindList = True):\n",
    "        self.blindListDisabled = noBlindList\n",
    "        self.filePath = f\"recordings/{ID}_{MODEL_NAME}/{ID}_{MODEL_NAME}_csvData/\"\n",
    "        self.fileNameList = [\"agentList.csv\", \"blindList.csv\", \"wallList.csv\", \"taskList.csv\"]\n",
    "        self.fileNameAndPathList =[]\n",
    "        for eachFileName in self.fileNameList:\n",
    "            self.fileNameAndPathList.append(self.filePath+eachFileName)\n",
    "            \n",
    "        if not os.path.isdir(self.filePath):\n",
    "            os.makedirs(self.filePath)\n",
    "        \n",
    "        self.agentListHeader = (\"Episode\",\"Step\",\"AgentID\",\"X\",\"Y\",\"Event\")\n",
    "        self.blindListHeader = (\"Episode\",\"Step\",\"AgentID\",\"X\",\"Y\")\n",
    "        self.wallListHeader = (\"X1\",\"Y1\",\"X2\",\"Y2\")\n",
    "        self.taskListHeader = (\"Episode\", \"Step\", \"X\", \"Y\", \"TaskExists\")\n",
    "        \n",
    "        self.write(self.agentListHeader, 0)\n",
    "        self.write(self.blindListHeader, 1)\n",
    "        self.write(self.wallListHeader, 2)\n",
    "        self.write(self.taskListHeader, 3)\n",
    "    \n",
    "    #stringID: \"agentList.csv\", \"blindList.csv\", \"wallList.csv\", \"taskList.csv\"\n",
    "    #intID: 0, 1, 2, 3\n",
    "    def write(self, data, intID = -1, stringID = \"-1\"):\n",
    "        if stringID != \"-1\":\n",
    "            i = 0\n",
    "            for eachString in self.fileNameList:\n",
    "                if eachString == stringID:\n",
    "                    intID = i\n",
    "                i+=1\n",
    "        if intID != -1 and not (self.blindListDisabled and intID == 1):\n",
    "            dataString = \"\"\n",
    "            for eachItem in data:\n",
    "                dataString+=f\"{eachItem},\"\n",
    "            dataString = f\"{dataString[:-1]}\\n\"\n",
    "            \n",
    "            with open(self.fileNameAndPathList[intID], 'a', encoding='utf8') as csvFile:\n",
    "                csvFile.write(dataString)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoombaEnv: \n",
    "    def __init__(self):\n",
    "        self.episode = 0\n",
    "        self.playerList=[]\n",
    "        self.trashList=[]\n",
    "        self.blockWallList = []\n",
    "        self.envMap = MapManager()\n",
    "        self.wallMap = WallManager()\n",
    "        self.spawnMap = SpawnManager()\n",
    "        self.blindMap = BlindSpotManager()\n",
    "        self.csv = CSVmanager()\n",
    "        \n",
    "        #Define map walls here\n",
    "        if WALL_ARRANGEMENT == \"Six Rooms\":\n",
    "            self.wallMap.addWall(10,20,\"Vertical\",9)\n",
    "            self.wallMap.addWall(23,20,\"Vertical\",9)\n",
    "            self.wallMap.addWall(10,9,\"Vertical\",9)\n",
    "            self.wallMap.addWall(23,9,\"Vertical\",9)\n",
    "\n",
    "            self.wallMap.addWall(-1,9,\"Horizontal\",5)\n",
    "            self.wallMap.addWall(5,9,\"Horizontal\",11)\n",
    "            self.wallMap.addWall(17,9,\"Horizontal\",11)\n",
    "            self.wallMap.addWall(29,9,\"Horizontal\",5)\n",
    "\n",
    "            self.wallMap.addWall(-1,11,\"Horizontal\",5)\n",
    "            self.wallMap.addWall(5,11,\"Horizontal\",11)\n",
    "            self.wallMap.addWall(17,11,\"Horizontal\",11)\n",
    "            self.wallMap.addWall(29,11,\"Horizontal\",5)\n",
    "        \n",
    "        if WALL_ARRANGEMENT == \"Four Room Block Wall\":\n",
    "            for y in range(0,9):\n",
    "                self.wallMap.addBlock(17,y)\n",
    "            for y in range(11,20):\n",
    "                self.wallMap.addBlock(17,y)\n",
    "            for x in range(0, 8):\n",
    "                self.wallMap.addBlock(x,8)\n",
    "                self.wallMap.addBlock(x,11)\n",
    "            for x in range(9, 26):\n",
    "                self.wallMap.addBlock(x,8)\n",
    "                self.wallMap.addBlock(x,11)\n",
    "            for x in range(27, 35):\n",
    "                self.wallMap.addBlock(x,8)\n",
    "                self.wallMap.addBlock(x,11)              \n",
    "            \n",
    "        if WALL_ARRANGEMENT == \"Custom\":\n",
    "            self.wallMap.addWall(SIZE_X//2-1,SIZE_Y//2+2,\"Vertical\",2)\n",
    "            self.wallMap.addWall(SIZE_X//2,SIZE_Y//2+1,\"Vertical\",1)\n",
    "            self.wallMap.addWall(SIZE_X//2,SIZE_Y//2+4,\"Vertical\",2)\n",
    "            self.wallMap.addWall(SIZE_X//2-1,SIZE_Y//2,\"Horizontal\",1)\n",
    "            self.wallMap.addWall(SIZE_X//2-1,SIZE_Y//2+2,\"Horizontal\",1)\n",
    "            self.wallMap.addWall(SIZE_X//2,SIZE_Y//2+1,\"Horizontal\",1)\n",
    "            self.wallMap.addWall(SIZE_X//2+1,SIZE_Y//2+3,\"Vertical\",2)\n",
    "            self.wallMap.addWall(SIZE_X//2,SIZE_Y//2+4,\"Horizontal\",1)\n",
    "            \n",
    "            #small room with 1 exit in center\n",
    "            #self.wallMap.addWall(SIZE_X//2-2,SIZE_Y//2+2,\"Vertical\",3)\n",
    "            #self.wallMap.addWall(SIZE_X//2+1,SIZE_Y//2+2,\"Vertical\",1)\n",
    "            #self.wallMap.addWall(SIZE_X//2+1,SIZE_Y//2,\"Vertical\",1)\n",
    "            #self.wallMap.addWall(SIZE_X//2-2,SIZE_Y//2-1,\"Horizontal\",3)\n",
    "            #self.wallMap.addWall(SIZE_X//2-2,SIZE_Y//2+2,\"Horizontal\",3)\n",
    "            \n",
    "            #2 horizontal lines in center\n",
    "            #self.wallMap.addWall(SIZE_X//2-3,SIZE_Y//2+1,\"Horizontal\",5)\n",
    "            #self.wallMap.addWall(SIZE_X//2-3,SIZE_Y//2,\"Horizontal\",5)\n",
    "            \n",
    "            #2 vertical lines in center\n",
    "            #self.wallMap.addWall(SIZE_X//2-1,SIZE_Y//2+3,\"Vertical\",5)\n",
    "            #self.wallMap.addWall(SIZE_X//2,SIZE_Y//2+3,\"Vertical\",5)\n",
    "        \n",
    "        if PRINT_CSV:\n",
    "            wallList = self.wallMap.getWallList()\n",
    "            for eachPair in wallList:\n",
    "                x1Str = str(eachPair[0][0])\n",
    "                y1Str = str(eachPair[0][1])\n",
    "                x2Str = str(eachPair[1][0])\n",
    "                y2Str = str(eachPair[1][1])\n",
    "                csvData = (x1Str, y1Str, x2Str, y2Str)\n",
    "                self.csv.write(csvData,2)\n",
    "            \n",
    "        #Define spawn chances here\n",
    "        if AGENT_SPAWN == \"4Room Center 8Agent Static\":\n",
    "            self.spawnMap.updateSpawnPlayerID(15,10,0)\n",
    "            self.spawnMap.updateSpawnPlayerID(15,9,1)\n",
    "            self.spawnMap.updateSpawnPlayerID(16,10,2)\n",
    "            self.spawnMap.updateSpawnPlayerID(16,9,3)\n",
    "            self.spawnMap.updateSpawnPlayerID(18,10,4)\n",
    "            self.spawnMap.updateSpawnPlayerID(18,9,5)\n",
    "            self.spawnMap.updateSpawnPlayerID(19,10,6)\n",
    "            self.spawnMap.updateSpawnPlayerID(19,9,7)\n",
    "        for x in range(0,SIZE_X):\n",
    "            for y in range(0,SIZE_Y):\n",
    "                if not self.wallMap.blockExists(x,y):\n",
    "                    if AGENT_SPAWN == \"4Room Center 8Agent Static\":\n",
    "                        #construction\n",
    "                        if (x >= 15 and x <= 19) and (y >= 9 and y <= 10):\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Player\", 100)\n",
    "                    \n",
    "                    if AGENT_SPAWN == \"4Room Center\":\n",
    "                        if (x >= 11 and x <= 23) and (y >= 9 and y <= 10):\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Player\", 100)\n",
    "                    if TASK_SPAWN == \"4Room Each Room\":\n",
    "                        if y < 8 or y > 11:\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Trash\", 100)\n",
    "                        \n",
    "                    if TASK_SPAWN == \"4Room 1Room Doubled\":\n",
    "                        if y < 8 or y > 11:\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Trash\", 50)\n",
    "                        if (y < 8) and (x < 17):\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Trash\", 100)\n",
    "                            \n",
    "                    if AGENT_SPAWN == \"Center\":\n",
    "                        if (x >= SIZE_X/2-2 and x <= SIZE_X/2+2) and (y >= SIZE_Y/2-2 and y <= SIZE_Y/2+2):\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Player\", 100)\n",
    "                    if TASK_SPAWN == \"Not Center\":\n",
    "                        if x < SIZE_X/2-2 or x > SIZE_X/2+2 or y < SIZE_Y/2-2 or y > SIZE_Y/2+2:\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Trash\", 100)\n",
    "\n",
    "                    if AGENT_SPAWN == \"Corridor\":\n",
    "                        if y > SIZE_Y/2-1 and y < SIZE_Y/2+1:\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Player\", 100)\n",
    "                    if TASK_SPAWN == \"Not Corridor\":\n",
    "                        if y < SIZE_Y/2-1 or y >= SIZE_Y/2+1:\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Trash\", 100)\n",
    "\n",
    "                    if TASK_SPAWN == \"Not Each Room\":\n",
    "                        eachRoomCoord = [\n",
    "                            (5, 5),\n",
    "                            (18, 5),\n",
    "                            (29, 5),\n",
    "                            (5, 15),\n",
    "                            (18, 15),\n",
    "                            (29, 15),\n",
    "                            (0, 10),\n",
    "                            (34, 9)\n",
    "                        ]\n",
    "                        match = False\n",
    "                        for eachCoord in eachRoomCoord:\n",
    "                            noSpawnX = eachCoord[0]\n",
    "                            noSpawnY = eachCoord[1]\n",
    "                            if noSpawnX == x and noSpawnY == y:\n",
    "                                match = True\n",
    "                        if not match:\n",
    "                            self.spawnMap.updateSpawn(x,y,\"Trash\", 100)\n",
    "\n",
    "                    if AGENT_SPAWN == \"Random\":\n",
    "                        self.spawnMap.updateSpawn(x,y,\"Player\", 100)\n",
    "                    if TASK_SPAWN == \"Random\":\n",
    "                        self.spawnMap.updateSpawn(x,y,\"Trash\", 100)\n",
    "                    \n",
    "        if AGENT_SPAWN == \"Each Room\":\n",
    "            eachRoomCoord = [\n",
    "                (5, 5),\n",
    "                (18, 5),\n",
    "                (29, 5),\n",
    "                (5, 15),\n",
    "                (18, 15),\n",
    "                (29, 15),\n",
    "                (0, 10),\n",
    "                (34, 9)\n",
    "            ]\n",
    "            for eachCoord in eachRoomCoord:\n",
    "                x = eachCoord[0]\n",
    "                y = eachCoord[1]\n",
    "                self.spawnMap.updateSpawn(x,y,\"Player\", 100)\n",
    "                \n",
    "        if TASK_SPAWN == \"Along Walls\":            \n",
    "            #Spawns trash next to walls            \n",
    "            wallCoords = self.wallMap.wallList\n",
    "            for eachPair in wallCoords:\n",
    "                pairX1 = eachPair[0][0]\n",
    "                pairY1 = eachPair[0][1]\n",
    "                pairX2 = eachPair[1][0]\n",
    "                pairY2 = eachPair[1][1]\n",
    "                self.spawnMap.updateSpawn(pairX1,pairY1,\"Trash\", 100)\n",
    "                self.spawnMap.updateSpawn(pairX2,pairY2,\"Trash\", 100)\n",
    "                #print(f\"({pairX1},{pairY1}) ({pairX2},{pairY2})\")\n",
    "        if AGENT_SPAWN == \"Custom\":\n",
    "            self.spawnMap.updateSpawn(SIZE_X//2,SIZE_Y//2,\"Player\", 100)\n",
    "        #if TASK_SPAWN == \"Custom\":\n",
    "            \n",
    "    def reset(self):\n",
    "        #Creates players in spawnable locations\n",
    "        self.playerList = [None] * PLAYER_COUNT\n",
    "        playerIDlist = list(range(PLAYER_COUNT))\n",
    "        random.shuffle(playerIDlist)\n",
    "        for eachID in playerIDlist:\n",
    "            retry = True\n",
    "            while retry:\n",
    "                retry = False\n",
    "                player = Roomba(obsRange = OBS_RANGE, viewType = VIEW_TYPE, ID=eachID)\n",
    "                \n",
    "                # Checks for duplicates\n",
    "                for eachPlayer in self.playerList:\n",
    "                    if not eachPlayer is None:\n",
    "                        if player == eachPlayer:\n",
    "                            retry = True\n",
    "                \n",
    "                # Checks if spawn was successful given probability of spawning in given location\n",
    "                if not self.spawnMap.trySpawn(player.x, player.y, \"Player\", eachID):\n",
    "                    retry = True\n",
    "                    \n",
    "            self.playerList[eachID] = copy.deepcopy(player)\n",
    "       \n",
    "        #Creates a list of trash with each trash having a unique location\n",
    "        self.trashList=[None]*TRASH_COUNT\n",
    "        trashIDlist = list(range(TRASH_COUNT))\n",
    "        random.shuffle(trashIDlist)\n",
    "        for eachID in trashIDlist:\n",
    "            retry = True\n",
    "            while retry:\n",
    "                trash = Trash()\n",
    "                retry = False\n",
    "            \n",
    "                # Checks for duplicates\n",
    "                for eachTrash in self.trashList:\n",
    "                    if not eachTrash is None:\n",
    "                        for eachPlayer in self.playerList:\n",
    "                            if trash == eachTrash or trash == eachPlayer:\n",
    "                                retry = True\n",
    "            \n",
    "                # Checks if spawn was successful given probability of spawning in given location\n",
    "                if not self.spawnMap.trySpawn(trash.x, trash.y, \"Trash\"):\n",
    "                    retry = True\n",
    "               \n",
    "            self.trashList[eachID] = copy.deepcopy(trash)\n",
    "            if PRINT_CSV:\n",
    "                csvData = (self.episode, self.playerList[0].stepsTaken, trash.x, trash.y, \"T\")\n",
    "                self.csv.write(csvData,3)\n",
    "        #Resets episode counter\n",
    "        self.episode_step = 0\n",
    "        \n",
    "        self.envMap = MapManager()\n",
    "        self.envMap.update(self.playerList, self.trashList, self.wallMap.blockWallList)\n",
    "        return self.envMap.getEnvironment()\n",
    "    \n",
    "    #Action is a number: 0,1,2,3 that represents an agent's action\n",
    "    def step(self, actionList):\n",
    "        self.episode_step += 1\n",
    "        \n",
    "        #Initialize lists with None values because agent data will be added in random order\n",
    "        rewardList = [None] * PLAYER_COUNT    \n",
    "    \n",
    "        agentIDlist = list(range(PLAYER_COUNT))\n",
    "        random.shuffle(agentIDlist)#randomizes action order of agents\n",
    "        for eachAgentID in agentIDlist:\n",
    "            i = eachAgentID\n",
    "            \n",
    "            #One player tries to move to another space\n",
    "            self.playerList[i].action(actionList[i])\n",
    "            \n",
    "            #As a default, assumes agent moved and assigns reward accordingly\n",
    "            reward = MOVE_PENALTY\n",
    "            self.playerList[i].event = \"Move\"\n",
    "            \n",
    "            #Handles agents reaching environment edges\n",
    "            if self.playerList[i].x < 0:\n",
    "                self.playerList[i].x = 0\n",
    "                reward = WALL_COLLISION_PENALTY\n",
    "                self.playerList[i].event = \"WallC\"\n",
    "                self.playerList[i].wallCollisionCount+=1\n",
    "            elif self.playerList[i].x > SIZE_X-1:\n",
    "                self.playerList[i].x = SIZE_X-1\n",
    "                reward = WALL_COLLISION_PENALTY\n",
    "                self.playerList[i].event = \"WallC\"\n",
    "                self.playerList[i].wallCollisionCount+=1\n",
    "            elif self.playerList[i].y < 0:\n",
    "                self.playerList[i].y = 0\n",
    "                reward = WALL_COLLISION_PENALTY\n",
    "                self.playerList[i].event = \"WallC\"\n",
    "                self.playerList[i].wallCollisionCount+=1\n",
    "            elif self.playerList[i].y > SIZE_Y-1:\n",
    "                self.playerList[i].y = SIZE_Y-1\n",
    "                reward = WALL_COLLISION_PENALTY\n",
    "                self.playerList[i].event = \"WallC\"\n",
    "                self.playerList[i].wallCollisionCount+=1\n",
    "            else:\n",
    "                #Handles wall and agent collisions\n",
    "                for eachPlayer in self.playerList:\n",
    "                    if (self.playerList[i] == eachPlayer and self.playerList[i].ID != eachPlayer.ID):\n",
    "                        reward = AGENT_COLLISION_PENALTY\n",
    "                        self.playerList[i].event = \"AgentC\"\n",
    "                        self.playerList[i].undo()\n",
    "                        self.playerList[i].agentCollisionCount+=1\n",
    "                    elif (self.wallMap.wallExists(eachPlayer.x, eachPlayer.y, eachPlayer.xBefore, eachPlayer.yBefore)):\n",
    "                        reward = WALL_COLLISION_PENALTY\n",
    "                        self.playerList[i].event = \"WallC\"\n",
    "                        self.playerList[i].undo()\n",
    "                        self.playerList[i].wallCollisionCount+=1\n",
    "            \n",
    "            #Overwrites reward if reaching trash\n",
    "            #Removes any trash on players position\n",
    "            for eachTrash in self.trashList:\n",
    "                if self.playerList[i] == eachTrash:\n",
    "                    self.playerList[i].tasksCompleted+=1\n",
    "                    reward = TRASH_REWARD\n",
    "                    self.playerList[i].event = \"Task\"\n",
    "                    self.trashList.remove(eachTrash)\n",
    "                eachTrash.count += 1#Increment counter for all existing trash\n",
    "    \n",
    "            #Saves data from this step for current player (except newObservations)\n",
    "            rewardList[i] = reward\n",
    "            \n",
    "        #Respawns any trash removed\n",
    "        while TRASH_COUNT > len(self.trashList):\n",
    "            newTrash = Trash()\n",
    "            retry = False\n",
    "            \n",
    "            #Checks if trash to spawn is in an empty space\n",
    "            for eachTrash in self.trashList:\n",
    "                for eachPlayer in self.playerList:\n",
    "                    if newTrash == eachTrash or newTrash == eachPlayer:\n",
    "                        retry = True\n",
    "            \n",
    "            # Checks if spawn was successful given probability of spawning in given location\n",
    "            if not self.spawnMap.trySpawn(newTrash.x, newTrash.y, \"Trash\"):\n",
    "                retry = True\n",
    "            \n",
    "            if not retry:\n",
    "                self.trashList.append(newTrash)\n",
    "                if PRINT_CSV:\n",
    "                    csvData = (self.episode, self.playerList[0].stepsTaken, newTrash.x, newTrash.y, \"T\")\n",
    "                    self.csv.write(csvData,3)\n",
    "        \n",
    "        # Creates a map of the environment\n",
    "        self.envMap = MapManager()\n",
    "        self.envMap.update(self.playerList, self.trashList, self.wallMap.blockWallList)\n",
    "        \n",
    "        # Checks if new observations shouldn't have main player position\n",
    "        removePlayer = False\n",
    "        if (VIEW_TYPE == \"Local\" or VIEW_TYPE == \"Merge\") and REMOVE_PLAYER_FROM_LOCAL_VIEW:\n",
    "            removePlayer = True\n",
    "        \n",
    "        #Agents take an action and the environment and each agents view of the environment is updated\n",
    "        newObservationList = [None]*PLAYER_COUNT\n",
    "        \n",
    "        agentIDlist = list(range(PLAYER_COUNT))\n",
    "        random.shuffle(agentIDlist)#randomizes action order of agents      \n",
    "        #Populates newObservationList\n",
    "        for eachAgentID in agentIDlist:\n",
    "            eachPlayer = self.playerList[eachAgentID]\n",
    "            if WALLS_BLOCK_VIEW:\n",
    "                blindList = self.blindMap.getBlindSpots(eachPlayer.x, eachPlayer.y, eachPlayer.obsRange, self.wallMap)\n",
    "            else:\n",
    "                blindList = None\n",
    "            \n",
    "            if PRINT_CSV and WALLS_BLOCK_VIEW:\n",
    "                for eachSpot in blindList:\n",
    "                    currentEpisode = str(self.episode)\n",
    "                    currentStep = str(self.episode_step)\n",
    "                    agentID = str(eachPlayer.ID)\n",
    "                    blindX = str(eachSpot[0])\n",
    "                    blindY = str(eachSpot[1])\n",
    "                    csvData = (currentEpisode, currentStep, agentID, blindX, blindY)\n",
    "                    self.csv.write(csvData,1)\n",
    "                    \n",
    "            #Create a new observation for each agent and append to list\n",
    "            newObservationList[eachAgentID] = copy.deepcopy(self.envMap.getView(OBS_RANGE, VIEW_TYPE, eachPlayer.ID, removePlayerPos = removePlayer, blindSpotList = blindList))\n",
    "        \n",
    "        #Checks if this episode is complete\n",
    "        done = False\n",
    "        if self.episode_step >= EPISODE_STEP_LIMIT:\n",
    "            done = True       \n",
    "            \n",
    "        # Returns new agent observations, whether this episode finished, and any data to be recorded for tensorflow\n",
    "        return newObservationList, rewardList, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScottDQNAgent:\n",
    "    def __init__(self):  \n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        \n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "        self.timeToAverageWeights = False\n",
    "    def setModel(self, model):\n",
    "        # Main model\n",
    "        self.model = model\n",
    "        \n",
    "        # Target network\n",
    "        self.target_model = get_single_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def create_model(self):\n",
    "        viewType = VIEW_TYPE\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(32, (2, 2), padding=\"same\",input_shape=OBSERVATION_SPACE_VALUES))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        #model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(64, (2, 2), padding=\"same\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        #model.add(Dropout(0.2))\n",
    "\n",
    "        #https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras\n",
    "        model.add(Flatten(input_shape=(5, 5, 16)))\n",
    "        model.add(Dense(100))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(ACTION_SPACE_SIZE, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (4)\n",
    "        \"\"\"\n",
    "        rmsProp = optimizers.RMSprop(lr=0.001, \n",
    "                                     rho=0.9, \n",
    "                                     epsilon=None, \n",
    "                                     decay=0.0)\n",
    "        \"\"\"\n",
    "        rmsProp = optimizers.RMSprop(RMSPROP_LEARNING_RATE)\n",
    "        model.compile(loss=\"mse\", optimizer=rmsProp, metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Adds step's data to a memory replay array\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "        \n",
    "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)\n",
    "        \n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            #self.target_model.set_weights(self.model.get_weights())\n",
    "            self.timeToAverageWeights = True\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape))[0]\n",
    "    \n",
    "#view range: view range\n",
    "#num: number of agents\n",
    "#input dim: number of channels\n",
    "#creates a set of models that share the same CNN/Maxpool layers\n",
    "def get_model():\n",
    "    model = []\n",
    "    inputs = Input(shape=OBSERVATION_SPACE_VALUES)\n",
    "    x = Conv2D(32, kernel_size=3, activation='relu', data_format=\"channels_last\")(inputs)\n",
    "    x = Conv2D(64, kernel_size=2, activation='relu', data_format=\"channels_last\")(x)\n",
    "    x = Flatten()(x)\n",
    "    for i in range(PLAYER_COUNT):\n",
    "        x1 = Dense(32, activation='relu')(x)\n",
    "        x1 = Dense(16, activation='relu')(x1)\n",
    "        y1 = Dense(5)(x1)\n",
    "        model1 = Model(inputs=inputs, outputs=y1)\n",
    "        model1.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=LEARNING_RATE))\n",
    "        # model1.summary()\n",
    "        model.append(model1)\n",
    "    return model\n",
    "\n",
    "def get_single_model():\n",
    "    inputs = Input(shape=OBSERVATION_SPACE_VALUES)\n",
    "    x = Conv2D(32, kernel_size=3, activation='relu', data_format=\"channels_last\")(inputs)\n",
    "    x = Conv2D(64, kernel_size=2, activation='relu', data_format=\"channels_last\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x1 = Dense(32, activation='relu')(x)\n",
    "    x1 = Dense(16, activation='relu')(x1)\n",
    "    y1 = Dense(5)(x1)\n",
    "    model1 = Model(inputs=inputs, outputs=y1)\n",
    "    model1.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=LEARNING_RATE))\n",
    "    # model1.summary()\n",
    "    return model1\n",
    "\n",
    "#gets a list of DQNagents and updates target weights\n",
    "def average_weights(agent_group):\n",
    "    n_layers = len(agent_group[0].model.get_weights())\n",
    "    avg_model_weights = list()\n",
    "    weights = [1/len(agent_group) for j in range(len(agent_group))]\n",
    "    for layer in range(n_layers):\n",
    "        layer_weights = np.array([agent.model.get_weights()[layer] for agent in agent_group])\n",
    "        avg_layer_weights = np.average(layer_weights, axis=0, weights=weights)\n",
    "        avg_model_weights.append(avg_layer_weights)\n",
    "    for i in range(len(agent_group)):\n",
    "        agent_group[i].target_model.set_weights(avg_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RoombaEnv()\n",
    "# Agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, loadModelID = -1):\n",
    "            \n",
    "        # Main model\n",
    "        self.model = self.create_model(loadModelID)\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = self.create_model(loadModelID = loadModelID, target = True)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # An array with last n steps for training\n",
    "        if LOAD_MODEL is not None:\n",
    "            dequeData = np.load(f\"recordings/{LOAD_MODEL}/models/{LOAD_MODEL}{loadModelID}.npy\", allow_pickle = True)\n",
    "            replayMem = deque(dequeData, maxlen =REPLAY_MEMORY_SIZE)\n",
    "        else:\n",
    "            replayMem = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        self.replay_memory = replayMem\n",
    "        \n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "    \n",
    "    def printModelDiagram(self, fileName = 'model.png'):\n",
    "        plot_model(self.model, show_shapes = True, to_file=f\"recordings/{ID}_{MODEL_NAME}/{fileName}\")\n",
    "    \n",
    "    def create_model(self, loadModelID = -1, target= False):\n",
    "        if LOAD_MODEL is not None:\n",
    "            if target:\n",
    "                modelPath = f\"recordings/{LOAD_MODEL}/models/{LOAD_MODEL}{loadModelID}_target.model\"\n",
    "            else:\n",
    "                modelPath = f\"recordings/{LOAD_MODEL}/models/{LOAD_MODEL}{loadModelID}.model\"\n",
    "            model = keras.models.load_model(modelPath)\n",
    "            return model\n",
    "        \n",
    "        if VIEW_TYPE == \"Merge\":\n",
    "            localInput = Input(shape=OBSERVATION_SPACE_VALUES)\n",
    "            x = Conv2D(32, (2, 2), padding=\"same\", data_format = \"channels_last\", input_shape=OBSERVATION_SPACE_VALUES)(localInput)\n",
    "            x = MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "            x = Conv2D(32, (2, 2), padding=\"same\")(x)\n",
    "            x = MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "            localModelEnd = Flatten()(x)\n",
    "            \n",
    "            relativeInput = Input(shape=OBSERVATION_SPACE_VALUES_TWO)\n",
    "            x = Conv2D(32, (2, 2), padding=\"same\", input_shape=OBSERVATION_SPACE_VALUES_TWO)(relativeInput)\n",
    "            x = MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "            x = Conv2D(32, (2, 2), padding=\"same\")(x)\n",
    "            x = MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "            relativeModelEnd = Flatten()(x)\n",
    "            \n",
    "            x = keras.layers.concatenate([localModelEnd, relativeModelEnd])\n",
    "            x = Dense(100, activation='relu')(x)\n",
    "            output = Dense(ACTION_SPACE_SIZE, activation='linear')(x)\n",
    "            \n",
    "            model = Model(inputs=[localInput, relativeInput], outputs = [output])\n",
    "            rmsProp = optimizers.RMSprop(LEARNING_RATE)\n",
    "            model.compile(loss=\"mse\", optimizer=rmsProp, metrics=['accuracy'])\n",
    "            #plot_model(model, to_file='model.png')\n",
    "        else:\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(32, (2, 2), padding=\"same\",input_shape=OBSERVATION_SPACE_VALUES))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "            #model.add(Dropout(0.2))\n",
    "\n",
    "            model.add(Conv2D(64, (2, 2), padding=\"same\"))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "            #model.add(Dropout(0.2))\n",
    "\n",
    "            #https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras\n",
    "            model.add(Flatten(input_shape=(5, 5, 16)))\n",
    "            model.add(Dense(100))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dense(ACTION_SPACE_SIZE, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (4)\n",
    "            \"\"\"\n",
    "            rmsProp = optimizers.RMSprop(lr=LEARNING_RATE, #lr=0.001 was default value\n",
    "                                         rho=0.9, \n",
    "                                         epsilon=None, \n",
    "                                         decay=0.0)  \n",
    "            \"\"\"\n",
    "            rmsProp = optimizers.RMSprop(LEARNING_RATE)\n",
    "            model.compile(loss=\"mse\", optimizer=rmsProp, metrics=['accuracy'])\n",
    "\n",
    "        return copy.deepcopy(model)\n",
    "    \n",
    "    # Adds step's data to a memory replay array\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(copy.deepcopy(transition))\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "     \n",
    "        # Each minibatch is a list of transitions\n",
    "        # Each transition is a list containing the following:\n",
    "        # transition[0] current state\n",
    "        # transition[1] action\n",
    "        # transition[2] reward\n",
    "        # transition[3] new state\n",
    "        # transition[4] done\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        # Get current states from minibatch, then query NN model for Q values    \n",
    "        if VIEW_TYPE == \"Merge\":\n",
    "            localCurrentStates = np.array([transition[0][0] for transition in minibatch])\n",
    "            relativeCurrentStates = np.array([transition[0][1] for transition in minibatch])\n",
    "            #relativeCurrentStates = np.expand_dims(relativeCurrentStates, axis=3) bookmark attempted fix\n",
    "            current_states = [localCurrentStates, relativeCurrentStates]\n",
    "        else:\n",
    "            current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        if VIEW_TYPE == \"Merge\":\n",
    "            localCurrentStates = np.array([transition[3][0] for transition in minibatch])\n",
    "            relativeCurrentStates = np.array([transition[3][1] for transition in minibatch])\n",
    "            #relativeCurrentStates = np.expand_dims(relativeCurrentStates, axis=3) bookmark attempted fix\n",
    "            new_current_states = [localCurrentStates, relativeCurrentStates]\n",
    "        else:\n",
    "            new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = [] #Stores each state, e.g., environment image, agent view\n",
    "        localX = []\n",
    "        relativeX = []\n",
    "        \n",
    "        y = [] #Stores action taken on a given state, e.g., up, down, left, right\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "            \n",
    "            # And append to our training data\n",
    "            if VIEW_TYPE == \"Merge\":\n",
    "                localState = np.array(current_state[0])\n",
    "                relativeState = np.array(current_state[1])\n",
    "                #relativeState = np.expand_dims(relativeState, axis=2)bookmark attempted fix\n",
    "                localX.append(localState)\n",
    "                relativeX.append(relativeState)\n",
    "            else:\n",
    "                X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "            \n",
    "        if VIEW_TYPE == \"Merge\":\n",
    "            self.model.fit([localX, relativeX], np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)\n",
    "        else:\n",
    "            self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)\n",
    "        \n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        if VIEW_TYPE == \"Merge\":\n",
    "            localViewState = state[0]\n",
    "            relativeViewState = state[1]\n",
    "            localViewInput = np.array(localViewState).reshape(-1, *localViewState.shape)\n",
    "            relativeViewInput = np.array(relativeViewState).reshape(-1, *relativeViewState.shape)\n",
    "            #print(relativeViewInput.shape)#bookmark possible fix for an issue\n",
    "            #relativeViewInput = np.expand_dims(relativeViewInput, axis=3)\n",
    "            #print(relativeViewInput.shape)#\n",
    "            return self.model.predict([localViewInput, relativeViewInput])[0]\n",
    "        else:\n",
    "            return self.model.predict(np.array(state).reshape(-1, *state.shape))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more repetitive results\n",
    "if RANDOM_SEED != -1:\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    #tf.random.set_seed(RANDOM_SEED)\n",
    "    tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|#######6  | 19134/25000 [144:12:50<45:34:16, 27.97s/episodes]"
     ]
    }
   ],
   "source": [
    "#Creates list of DQNs for agents\n",
    "agentList = [None]*PLAYER_COUNT \n",
    "agentIDlist = list(range(PLAYER_COUNT))\n",
    "random.shuffle(agentIDlist)\n",
    "if USE_SCOTT_MODEL:\n",
    "    modelList = get_model()\n",
    "\n",
    "for eachPlayerID in agentIDlist:\n",
    "    loadModelPlayerID = eachPlayerID\n",
    "    if not USE_SCOTT_MODEL:\n",
    "        agentList[eachPlayerID] = copy.deepcopy(DQNAgent(loadModelPlayerID))\n",
    "    else:\n",
    "        newAgent = ScottDQNAgent()\n",
    "        newAgent.setModel(modelList[loadModelPlayerID])\n",
    "        newAgent.replay_memory = GLOBAL_REPLAY_MEMORY\n",
    "        agentList[eachPlayerID] = copy.deepcopy(newAgent)\n",
    "    \n",
    "# Creates a text file with the parameters for this run\n",
    "if RECORD_PARAMETERS:\n",
    "    paramPath = f'recordings/{ID}_{MODEL_NAME}/'\n",
    "    fileName = str(MODEL_NAME) + \"-\" + str(ID) + \".txt\"\n",
    "    fileContents = f\"\"\"\n",
    "        #Parameters\n",
    "        #Misc Parameters\n",
    "        ID = {ID}\n",
    "        MODEL_NAME = {MODEL_NAME}\n",
    "        RANDOM_SEED = {RANDOM_SEED}\n",
    "        OBSERVATION_SPACE_VALUES = {OBSERVATION_SPACE_VALUES}\n",
    "        ACTION_SPACE_SIZE = {ACTION_SPACE_SIZE}\n",
    "\n",
    "        #Model Settings\n",
    "        DISCOUNT = {DISCOUNT}\n",
    "        REPLAY_MEMORY_SIZE = {REPLAY_MEMORY_SIZE}  # How many last steps to keep for model training\n",
    "        MIN_REPLAY_MEMORY_SIZE = {MIN_REPLAY_MEMORY_SIZE}  # Minimum number of steps in a memory to start training\n",
    "        MINIBATCH_SIZE = {MINIBATCH_SIZE}  # How many steps (samples) to use for training\n",
    "        UPDATE_TARGET_EVERY = {UPDATE_TARGET_EVERY}  # Terminal states (end of episodes)\n",
    "        LEARNING_RATE = {LEARNING_RATE}\n",
    "\n",
    "        # Agent View Settings\n",
    "        REMOVE_PLAYER_FROM_LOCAL_VIEW = {REMOVE_PLAYER_FROM_LOCAL_VIEW}\n",
    "        WALLS_BLOCK_VIEW = {WALLS_BLOCK_VIEW}\n",
    "        VIEW_TYPE = {VIEW_TYPE} # Relative or Local\n",
    "\n",
    "        # Exploration settings\n",
    "        EPSILON_DECAY = {EPSILON_DECAY}\n",
    "        MIN_EPSILON = {MIN_EPSILON}\n",
    "\n",
    "        # Environment settings\n",
    "        START_EPISODE = {START_EPISODE}\n",
    "        END_EPISODE = {END_EPISODE}\n",
    "        OBS_RANGE = {OBS_RANGE}\n",
    "        SIZE_X = {SIZE_X}#Environment width\n",
    "        SIZE_Y = {SIZE_Y}#Environment height\n",
    "        EPISODE_STEP_LIMIT = {EPISODE_STEP_LIMIT}\n",
    "        PLAYER_COUNT = {PLAYER_COUNT}\n",
    "        TRASH_COUNT = {TRASH_COUNT}\n",
    "        AGENT_SPAWN = {AGENT_SPAWN} # \"Random\", \"Center\", \"Custom\"\n",
    "        TASK_SPAWN = {TASK_SPAWN} # \"Random\", \"Not Center\", \"Along Walls\", \"Custom\"\n",
    "        WALL_ARRANGEMENT = {WALL_ARRANGEMENT} # \"Six Rooms\", \"Custom\"\n",
    "\n",
    "        #Key settings (the number to represent each thing in the DQN matrix)\n",
    "        PLAYER_N = {PLAYER_N}  # player key \n",
    "        TRASH_N = {TRASH_N}  # trash key\n",
    "        OTHERS_N = {OTHERS_N} # Other players key\n",
    "\n",
    "        #Reward Scheme\n",
    "        MOVE_PENALTY = {MOVE_PENALTY} #make negative\n",
    "        WALL_COLLISION_PENALTY = {WALL_COLLISION_PENALTY} #make negative\n",
    "        AGENT_COLLISION_PENALTY = {AGENT_COLLISION_PENALTY} #make negative\n",
    "        TRASH_REWARD = {TRASH_REWARD}\n",
    "        \"\"\"\n",
    "    #{str_model_summary}\n",
    "    #Writes model summary to file\n",
    "    if not os.path.isdir(paramPath):\n",
    "        os.makedirs(paramPath)\n",
    "    with open(paramPath + fileName, \"w+\") as f:\n",
    "        with redirect_stdout(f):\n",
    "            agentList[0].model.summary()\n",
    "    f=open(paramPath + fileName, \"a\")\n",
    "    f.write(fileContents)\n",
    "    f.close()\n",
    "    \n",
    "    if not USE_SCOTT_MODEL:\n",
    "        agentList[0].printModelDiagram()\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(START_EPISODE, END_EPISODE + 1), ascii=True, unit='episodes'):\n",
    "    env.episode = episode\n",
    "    # Restarting episode - reset total values and arrays for data recording and step number\n",
    "    step = 1\n",
    "    # Reset environment and get initial state\n",
    "    env.reset()\n",
    "    envMap = MapManager()\n",
    "    envMap.update(env.playerList, env.trashList, env.wallMap.blockWallList)\n",
    "    currentStateList = [None]*PLAYER_COUNT# List that contains the current state for each player\n",
    "    playerIDlist = list(range(PLAYER_COUNT))\n",
    "    random.shuffle(playerIDlist)\n",
    "    for eachPlayerID in playerIDlist:\n",
    "        #Checks if \"player\" channels should be removed\n",
    "        removePlayer = False\n",
    "        if (VIEW_TYPE == \"Local\" or VIEW_TYPE == \"Merge\") and REMOVE_PLAYER_FROM_LOCAL_VIEW:\n",
    "            removePlayer = True\n",
    "        \n",
    "        #Gets blind spts for each player if ray casting is on (just for initial episode step)\n",
    "        eachPlayer = env.playerList[eachPlayerID]\n",
    "        if WALLS_BLOCK_VIEW:\n",
    "            blindList = env.blindMap.getBlindSpots(eachPlayer.x, eachPlayer.y, eachPlayer.obsRange, env.wallMap)\n",
    "        else:\n",
    "            blindList = None\n",
    "        \n",
    "        #Gets the current state of each player (just for initial episode step)\n",
    "        currentStateList[eachPlayerID] = envMap.getView(OBS_RANGE, VIEW_TYPE, eachPlayer.ID, removePlayerPos = removePlayer, blindSpotList = blindList)\n",
    "        \n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "        actionList = [None]*PLAYER_COUNT\n",
    "        playerIDlist = list(range(PLAYER_COUNT))\n",
    "        random.shuffle(playerIDlist)\n",
    "        \n",
    "        #Determines which action each agent takes\n",
    "        for eachPlayerID in playerIDlist:\n",
    "            # This part stays mostly the same, the change is to query a model for Q values\n",
    "            if np.random.random() > epsilon:\n",
    "                # Get action from Q table\n",
    "                action = np.argmax(agentList[eachPlayerID].get_qs(currentStateList[eachPlayerID]))\n",
    "            else:\n",
    "                # Get random action\n",
    "                action = np.random.randint(0, ACTION_SPACE_SIZE)\n",
    "            actionList[eachPlayerID] = copy.deepcopy(action)\n",
    "            \n",
    "        #Changes the environment according to agent actions\n",
    "        new_stateList, rewardList, done = env.step(actionList)\n",
    "        \n",
    "        if CHECK_DQN_INPUT and step == 5:\n",
    "            np.set_printoptions(threshold=sys.maxsize)\n",
    "            print(\"Environment\")\n",
    "            print(env.envMap.printEnv())#bookmark debug\n",
    "            print(\"\")\n",
    "            print(\"agent 0 state\")\n",
    "            if VIEW_TYPE == \"Merge\":\n",
    "                print(\"Local view portion\")\n",
    "                envMap.printView(new_stateList[0][0])\n",
    "                print(\"Relative view portion\")\n",
    "                envMap.printView(new_stateList[0][1])\n",
    "            else:\n",
    "                envMap.printView(new_stateList[0])\n",
    "            np.set_printoptions(threshold=1000)\n",
    "        \n",
    "        #Updates CSV files\n",
    "        if PRINT_CSV:\n",
    "            episodeStr = str(episode)\n",
    "            for i in range(0,len(env.playerList)):\n",
    "                agentStepsTaken = str(env.playerList[i].stepsTaken)\n",
    "                agentID = str(env.playerList[i].ID)\n",
    "                agentX = str(env.playerList[i].x)\n",
    "                agentY = str(env.playerList[i].y)\n",
    "                agentEvent = env.playerList[i].event\n",
    "                csvData = (episodeStr, agentStepsTaken, agentID, agentX, agentY, agentEvent)\n",
    "                env.csv.write(csvData, 0)\n",
    "                \n",
    "                if agentEvent == \"Task\":\n",
    "                    csvData = (episodeStr, agentStepsTaken, agentX, agentY, \"F\")\n",
    "                    env.csv.write(csvData, 3)\n",
    "        \n",
    "        timeToAverageWeightsChecker = []\n",
    "        playerIDlist = list(range(PLAYER_COUNT))\n",
    "        random.shuffle(playerIDlist)\n",
    "        for eachPlayerID in playerIDlist:\n",
    "            # Every step we update replay memory and train main network\n",
    "            agentList[eachPlayerID].update_replay_memory((currentStateList[eachPlayerID], actionList[eachPlayerID], rewardList[eachPlayerID], new_stateList[eachPlayerID], done))\n",
    "            agentList[eachPlayerID].train(done, step)\n",
    "            if USE_SCOTT_MODEL:\n",
    "                timeToAverageWeightsChecker.append(agentList[eachPlayerID].timeToAverageWeights)\n",
    "        \n",
    "        if USE_SCOTT_MODEL and timeToAverageWeightsChecker[0]:\n",
    "            average_weights(agentList)\n",
    "            for agent in agentList:\n",
    "                agent.timeToAverageWeights = False\n",
    "        \n",
    "        currentStateList = copy.deepcopy(new_stateList)\n",
    "        step += 1\n",
    "    \n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "    \n",
    "    # Save model\n",
    "    if SAVE_MODEL_EVERY != -1 and not episode % SAVE_MODEL_EVERY:\n",
    "        playerIDlist = list(range(PLAYER_COUNT))\n",
    "        basePath = f'recordings/{ID}_{MODEL_NAME}/models/'\n",
    "        for eachPlayerID in playerIDlist:\n",
    "            if not os.path.isdir(basePath):\n",
    "                os.makedirs(basePath)\n",
    "            agentList[eachPlayerID].model.save(f\"{basePath}{ID}_{MODEL_NAME}{eachPlayerID}.model\")\n",
    "            agentList[eachPlayerID].target_model.save(f\"{basePath}{ID}_{MODEL_NAME}{eachPlayerID}_target.model\")\n",
    "            \n",
    "            replayMem = agentList[eachPlayerID].replay_memory#bookmark havent tested loading portion\n",
    "            np.save(f\"{basePath}{ID}_{MODEL_NAME}{eachPlayerID}\", replayMem, allow_pickle = True)\n",
    "            \n",
    "        paramFileName = str(MODEL_NAME) + \"-\" + str(ID) + \".txt\"\n",
    "        paramFileContents = f\"\"\"\n",
    "                            Next episode: {episode+1}\n",
    "                            Next epsilon: {epsilon}\n",
    "                            \"\"\"\n",
    "        if not os.path.isdir(basePath):\n",
    "            os.makedirs(basePath)\n",
    "        f=open(basePath + paramFileName, \"w+\")\n",
    "        f.write(paramFileContents)\n",
    "        f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
